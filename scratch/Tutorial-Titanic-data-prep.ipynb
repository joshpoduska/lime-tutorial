{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup ------------------------------------------------------------------------------------\n",
    "### Libraries, Functions, and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.6.2 |Anaconda custom (64-bit)| (default, Jul 20 2017, 13:51:32) \n",
      "[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)]\n",
      "scikit-learn version: 0.19.1\n"
     ]
    }
   ],
   "source": [
    "import sklearn as sk\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "from pylab import *\n",
    "import sys\n",
    "\n",
    "print ('Python version:', sys.version)\n",
    "print ('scikit-learn version:', sk.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a Data Quality Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#input a dataframe\n",
    "#returns a dataframe with helpful statistics on the dataframe and its columns\n",
    "\n",
    "def dqr(d):\n",
    "    #data types\n",
    "    dqr_data_types = pd.DataFrame(d.dtypes, columns=['Data Type'])\n",
    "\n",
    "    #percent missing\n",
    "    dqr_percent_missing = pd.DataFrame(100*(d.isnull().sum()/len(d)).round(3), columns=['% Missing'])\n",
    "\n",
    "    #unique values\n",
    "    dqr_unique_values = pd.DataFrame(columns=['Unique Values'])\n",
    "    for c in d:\n",
    "        dqr_unique_values.loc[c]=d[c].nunique()\n",
    "        \n",
    "    #mode\n",
    "    dqr_mode = pd.DataFrame(d.mode().loc[0], columns=['Mode'])\n",
    "    \n",
    "    #count mode\n",
    "    dqr_count_mode = pd.DataFrame(columns=['Count Mode'])\n",
    "    for c in d:\n",
    "        dqr_count_mode.loc[c]=d[c][d[c] == dqr_mode.loc[[c]].iloc[0]['Mode']].count()  \n",
    "\n",
    "    #% mode\n",
    "    dqr_percent_mode = pd.DataFrame(100*(dqr_count_mode['Count Mode'].values/len(d)).round(3), index=dqr_count_mode.index, columns=['% Mode'])\n",
    "            \n",
    "    #distribution stats    \n",
    "    i=1\n",
    "    for c in d:\n",
    "        if i==1:\n",
    "            dqr_stats = pd.DataFrame(d[c].describe())\n",
    "        if i>1:\n",
    "            dqr_stats = dqr_stats.join(pd.DataFrame(d[c].describe()))\n",
    "        i=i+1\n",
    "    dqr_stats=dqr_stats.transpose().drop('count', axis=1)\n",
    "\n",
    "    print(\"# of records: \", len(d))\n",
    "    \n",
    "    #don't include count mode\n",
    "    #.join(dqr_count_mode[['Count Mode']].astype(int))\n",
    "    \n",
    "    return dqr_data_types.join(dqr_unique_values[['Unique Values']].astype(int)).join(dqr_percent_missing).join(dqr_mode).join(dqr_percent_mode).join(dqr_stats)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a Cross Validation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import cross_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# input a list of models, the type of score to be used with cv, and k\n",
    "# each element in the list of models should have two items: the model object and the name you want to use for that \n",
    "# model object\n",
    "# returns a dataframe with the names you entered and the mean of the cv scores across all k folds\n",
    "\n",
    "def cv_fun(models, score, k):\n",
    "    i = 0\n",
    "    for m in models:\n",
    "        scores = cross_validation.cross_val_score(models[i][0], titanic_inputs, titanic_target, scoring=score, cv=k)\n",
    "    \n",
    "        if i==0:\n",
    "            list1 = list()\n",
    "            list2 = list()\n",
    "            \n",
    "        list1.append(scores.mean())\n",
    "        list2.append(models[i][1])\n",
    "\n",
    "        i=i+1\n",
    "\n",
    "    return pd.DataFrame(list1, index=list2, columns=[score]).sort_values(by=score, ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'train.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-9f8c35feba73>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtitanic_raw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtitanic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtitanic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    653\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    654\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 655\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    656\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    403\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 405\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    762\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 764\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    765\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m    983\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1603\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'allow_leading_cols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1604\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1605\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1607\u001b[0m         \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__ (pandas/_libs/parsers.c:4209)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source (pandas/_libs/parsers.c:8873)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File b'train.csv' does not exist"
     ]
    }
   ],
   "source": [
    "titanic_raw = pd.read_csv('train.csv', header=0, index_col=0)\n",
    "titanic = pd.read_csv('train.csv', header=0, index_col=0)\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Data Prep --------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the DQR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dqr(titanic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop Ticket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "titanic = titanic.drop(['Ticket'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change Sex to a dummie field called Male"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "titanic['Male'] = titanic['Sex'].map( {'female': 0, 'male': 1} ).astype(int)\n",
    "titanic = titanic.drop(['Sex'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create FamilySize from SibSp and Parch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "titanic['FamilySize'] = titanic['SibSp'] + titanic['Parch']\n",
    "titanic = titanic.drop(['SibSp', 'Parch'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore Cabin and create CabinFill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "titanic['Cabin'].value_counts().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "titanic.Cabin.str.get(0).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "titanic['CabinFill'] = titanic.Cabin.str.get(0)\n",
    "titanic.loc[ (titanic.CabinFill.isnull()) , 'CabinFill'] = \"NULL\"\n",
    "titanic = titanic.drop(['Cabin'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill nulls for Embarked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "titanic.loc[ (titanic.Embarked.isnull()), 'Embarked'] = \"NULL\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "titanic['Title']=\"NONE\"\n",
    "titanic.loc[ (titanic.Name.str.contains(\"Mr. \")), 'Title'] = \"Mr\"\n",
    "titanic.loc[ (titanic.Name.str.contains(\"Mrs. \")), 'Title'] = \"Mrs\"\n",
    "titanic.loc[ (titanic.Name.str.contains(\"Miss. \")), 'Title'] = \"Miss\"\n",
    "titanic.loc[ (titanic.Name.str.contains(\"Master. \")), 'Title'] = \"Master\"\n",
    "titanic.loc[ (titanic.Name.str.contains(\"Rev. \")), 'Title'] = \"Rev\"\n",
    "titanic.loc[ (titanic.Name.str.contains(\"Mme. \")), 'Title'] = \"Mme\"\n",
    "titanic = titanic.drop(['Name'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dummie fields for the remaining categorical fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d1 = pd.get_dummies(titanic.Title,prefix='Title') #, drop_first=True)\n",
    "d2 = pd.get_dummies(titanic.CabinFill,prefix='Cabin') #, drop_first=True)\n",
    "d3 = pd.get_dummies(titanic.Embarked,prefix='Emb') #, drop_first=True)\n",
    "d4 = pd.get_dummies(titanic.Pclass,prefix='Pclass') #, drop_first=True)\n",
    "titanic=pd.concat([titanic,d1,d2,d3,d4],axis=1)\n",
    "titanic = titanic.drop(['Title', 'CabinFill', 'Embarked', 'Pclass'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use a linear model to impute Age when it is missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainage = titanic[ titanic['Age'].isnull()==0 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lr = LinearRegression()\n",
    "columns = titanic.loc[:, titanic.columns != 'Age'].columns\n",
    "target = trainage['Age'].values\n",
    "trainX = trainage[list(columns)].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr.fit(trainX,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainage.loc[:,'AgePred']=lr.predict(trainX)\n",
    "plt.axis([0.0,90.0, 0.0,90.0])\n",
    "ax = plt.gca()\n",
    "ax.set_autoscale_on(False)\n",
    "plt.scatter(trainage.Age,trainage.AgePred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pAge = pd.DataFrame(lr.predict(titanic[columns]), index=titanic.index, columns=['Age Pred'])\n",
    "titanic=pd.concat([titanic,pAge],axis=1)\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "titanic.loc[titanic['Age'].isnull(), 'Age'] = titanic['Age Pred']\n",
    "titanic = titanic.drop(['Age Pred'],axis=1)\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dqr(titanic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Predictive Modeling Approach --------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The data definitely has some correlated inputs (e.g. Age and FamilySize)\n",
    "#### Ignore those for now as that does not effect ability to predict\n",
    "#### Many of the categorical fields are almost always zero making the input space cluttered\n",
    "#### The ensemble methods will not be hampered by those\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "titanic_train = titanic.sample(frac=.70, random_state=0)\n",
    "titanic_train.sort_index(inplace=True)\n",
    "titanic_test = titanic[~titanic.isin(titanic_train)].dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Begin Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "columns = list(titanic.columns)\n",
    "columns.remove('Survived')\n",
    "columns.remove('Title_NONE')\n",
    "columns.remove('Cabin_NULL')\n",
    "columns.remove('Emb_NULL')\n",
    "columns.remove('Pclass_1')\n",
    "titanic_target = titanic_train[\"Survived\"].values\n",
    "titanic_inputs = titanic_train[columns].values\n",
    "titanic_test_target = titanic_test[\"Survived\"].values\n",
    "titanic_test_inputs = titanic_test[columns].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "dt1 = tree.DecisionTreeClassifier(criterion='gini', max_depth=5,min_samples_leaf=10)\n",
    "dt1 = dt1.fit(titanic_inputs, titanic_target)\n",
    "dt1prb = dt1.predict_proba(titanic_inputs)\n",
    "\n",
    "dt2 = tree.DecisionTreeClassifier(criterion='gini', max_depth=3,min_samples_leaf=5)\n",
    "dt2 = dt2.fit(titanic_inputs, titanic_target)\n",
    "dt2prb = dt2.predict_proba(titanic_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "nb1 = GaussianNB()\n",
    "nb1 = nb1.fit(titanic_inputs, titanic_target)\n",
    "nb1prb = nb1.predict_proba(titanic_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "sv1 = svm.SVC(kernel='linear', probability=True)\n",
    "sv1 = sv1.fit(titanic_inputs, titanic_target)\n",
    "sv1prb = sv1.predict_proba(titanic_inputs)\n",
    "\n",
    "sv2 = svm.SVC(kernel='rbf', probability=True)\n",
    "sv2 = sv2.fit(titanic_inputs, titanic_target)\n",
    "sv2prb = sv2.predict_proba(titanic_inputs)\n",
    "\n",
    "sv3 = svm.SVC(kernel='sigmoid', probability=True)\n",
    "sv3 = sv3.fit(titanic_inputs, titanic_target)\n",
    "sv3prb = sv3.predict_proba(titanic_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "kn1 = KNeighborsClassifier(n_neighbors=1)\n",
    "kn1 = kn1.fit(titanic_inputs, titanic_target)\n",
    "kn1prb = kn1.predict_proba(titanic_inputs)\n",
    "\n",
    "kn2 = KNeighborsClassifier(n_neighbors=5)\n",
    "kn2 = kn2.fit(titanic_inputs, titanic_target)\n",
    "kn2prb = kn2.predict_proba(titanic_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "ad1 = AdaBoostClassifier(n_estimators=1000)\n",
    "ad1 = ad1.fit(titanic_inputs, titanic_target)\n",
    "ad1prb = ad1.predict_proba(titanic_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier \n",
    "rf1 = RandomForestClassifier(n_estimators = 10)\n",
    "rf1 = rf1.fit(titanic_inputs, titanic_target)\n",
    "rf1prb = rf1.predict_proba(titanic_inputs)\n",
    "rf1pclass = rf1.predict(titanic_inputs)\n",
    "\n",
    "rf2 = RandomForestClassifier(n_estimators = 100)\n",
    "rf2 = rf2.fit(titanic_inputs, titanic_target)\n",
    "rf2prb = rf2.predict_proba(titanic_inputs)\n",
    "rf2pclass = rf2.predict(titanic_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC Curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### On all the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "dt1fpr,dt1tpr,dt1thresholds = metrics.roc_curve(titanic_target,dt1prb[:,1])\n",
    "dt2fpr,dt2tpr,dt2thresholds = metrics.roc_curve(titanic_target,dt2prb[:,1])\n",
    "rf1fpr,rf1tpr,rf1thresholds = metrics.roc_curve(titanic_target,rf1prb[:,1])\n",
    "rf2fpr,rf2tpr,rf2thresholds = metrics.roc_curve(titanic_target,rf2prb[:,1])\n",
    "sv1fpr,sv1tpr,sv1thresholds = metrics.roc_curve(titanic_target,sv1prb[:,1])\n",
    "sv2fpr,sv2tpr,sv2thresholds = metrics.roc_curve(titanic_target,sv2prb[:,1])\n",
    "sv3fpr,sv3tpr,sv3thresholds = metrics.roc_curve(titanic_target,sv3prb[:,1])\n",
    "kn1fpr,kn1tpr,kn1thresholds = metrics.roc_curve(titanic_target,kn1prb[:,1])\n",
    "kn2fpr,kn2tpr,kn2thresholds = metrics.roc_curve(titanic_target,kn2prb[:,1])\n",
    "ad1fpr,ad1tpr,ad1thresholds = metrics.roc_curve(titanic_target,ad1prb[:,1])\n",
    "nb1fpr,nb1tpr,nb1thresholds = metrics.roc_curve(titanic_target,nb1prb[:,1])\n",
    "\n",
    "plt.clf()\n",
    "plt.plot(dt1fpr,dt1tpr,color='red')\n",
    "plt.plot(dt2fpr,dt2tpr,color='yellow')\n",
    "plt.plot(rf1fpr,rf1tpr,color='blue')\n",
    "plt.plot(rf2fpr,rf2tpr,color='purple')\n",
    "plt.plot(sv1fpr,sv1tpr,color='green')\n",
    "plt.plot(sv2fpr,sv2tpr,color='cyan')\n",
    "plt.plot(sv3fpr,sv3tpr,color='grey')\n",
    "plt.plot(kn2fpr,kn2tpr,color='black')\n",
    "plt.plot(kn1fpr,kn1tpr,color='purple')\n",
    "plt.plot(ad1fpr,ad1tpr,color='magenta')\n",
    "plt.plot(nb1fpr,nb1tpr,color='brown')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run 5-fold cv to estimate predictive power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use the cv function found up under the Setup section\n",
    "# enter a list with each entry holding the model object followed by a text name you want to give the model\n",
    "\n",
    "input_models = [[rf1, 'rf1']]\n",
    "input_models.append([rf2, 'rf2'])\n",
    "input_models.append([sv1, 'sv1'])\n",
    "input_models.append([sv2, 'sv2'])\n",
    "input_models.append([sv3, 'sv3'])\n",
    "input_models.append([nb1, 'nb1'])\n",
    "input_models.append([ad1, 'ad1'])\n",
    "input_models.append([kn1, 'kn1'])\n",
    "input_models.append([kn2, 'kn2'])\n",
    "input_models.append([dt1, 'dt1'])\n",
    "input_models.append([dt2, 'dt2'])\n",
    "\n",
    "cv_roc = cv_fun(input_models, 'roc_auc', 5)\n",
    "cv_roc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#build a dataframe with predictive results\n",
    "\n",
    "#dataframe index\n",
    "auc_test_names = list()\n",
    "auc_test_names.append('dt1')\n",
    "auc_test_names.append('dt2')\n",
    "auc_test_names.append('rf1')\n",
    "auc_test_names.append('rf2')\n",
    "auc_test_names.append('sv1')\n",
    "auc_test_names.append('sv2')\n",
    "auc_test_names.append('sv3')\n",
    "auc_test_names.append('kn1')\n",
    "auc_test_names.append('kn2')\n",
    "auc_test_names.append('ad1')\n",
    "auc_test_names.append('nb1')\n",
    "\n",
    "dt1fpr_test,dt1tpr_test,dt1thresholds_test = \\\n",
    "    metrics.roc_curve(titanic_test_target,dt1.predict_proba(titanic_test_inputs)[:,1])\n",
    "dt2fpr_test,dt2tpr_test,dt2thresholds_test = \\\n",
    "    metrics.roc_curve(titanic_test_target,dt2.predict_proba(titanic_test_inputs)[:,1])\n",
    "rf1fpr_test,rf1tpr_test,rf1thresholds_test = \\\n",
    "    metrics.roc_curve(titanic_test_target,rf1.predict_proba(titanic_test_inputs)[:,1])\n",
    "rf2fpr_test,rf2tpr_test,rf2thresholds_test = \\\n",
    "    metrics.roc_curve(titanic_test_target,rf2.predict_proba(titanic_test_inputs)[:,1])\n",
    "sv1fpr_test,sv1tpr_test,sv1thresholds_test = \\\n",
    "    metrics.roc_curve(titanic_test_target,sv1.predict_proba(titanic_test_inputs)[:,1])\n",
    "sv2fpr_test,sv2tpr_test,sv2thresholds_test = \\\n",
    "    metrics.roc_curve(titanic_test_target,sv2.predict_proba(titanic_test_inputs)[:,1])\n",
    "sv3fpr_test,sv3tpr_test,sv3thresholds_test = \\\n",
    "    metrics.roc_curve(titanic_test_target,sv3.predict_proba(titanic_test_inputs)[:,1])\n",
    "kn1fpr_test,kn1tpr_test,kn1thresholds_test = \\\n",
    "    metrics.roc_curve(titanic_test_target,kn1.predict_proba(titanic_test_inputs)[:,1])\n",
    "kn2fpr_test,kn2tpr_test,kn2thresholds_test = \\\n",
    "    metrics.roc_curve(titanic_test_target,kn2.predict_proba(titanic_test_inputs)[:,1])\n",
    "ad1fpr_test,ad1tpr_test,ad1thresholds_test = \\\n",
    "    metrics.roc_curve(titanic_test_target,ad1.predict_proba(titanic_test_inputs)[:,1])\n",
    "nb1fpr_test,nb1tpr_test,nb1thresholds_test = \\\n",
    "    metrics.roc_curve(titanic_test_target,nb1.predict_proba(titanic_test_inputs)[:,1])\n",
    "\n",
    "auc_test = list() \n",
    "auc_test.append(metrics.auc(dt1fpr_test, dt1tpr_test))\n",
    "auc_test.append(metrics.auc(dt2fpr_test, dt2tpr_test))\n",
    "auc_test.append(metrics.auc(rf1fpr_test, rf1tpr_test))\n",
    "auc_test.append(metrics.auc(rf2fpr_test, rf2tpr_test))\n",
    "auc_test.append(metrics.auc(sv1fpr_test, sv1tpr_test))\n",
    "auc_test.append(metrics.auc(sv2fpr_test, sv2tpr_test))\n",
    "auc_test.append(metrics.auc(sv3fpr_test, sv3tpr_test))\n",
    "auc_test.append(metrics.auc(kn1fpr_test, kn1tpr_test))\n",
    "auc_test.append(metrics.auc(kn2fpr_test, kn2tpr_test))\n",
    "auc_test.append(metrics.auc(ad1fpr_test, ad1tpr_test))\n",
    "auc_test.append(metrics.auc(nb1fpr_test, nb1tpr_test))\n",
    "\n",
    "acc_test = list() \n",
    "acc_test.append(metrics.accuracy_score(titanic_test_target,dt1.predict(titanic_test_inputs)))\n",
    "acc_test.append(metrics.accuracy_score(titanic_test_target,dt2.predict(titanic_test_inputs)))\n",
    "acc_test.append(metrics.accuracy_score(titanic_test_target,rf1.predict(titanic_test_inputs)))\n",
    "acc_test.append(metrics.accuracy_score(titanic_test_target,rf2.predict(titanic_test_inputs)))\n",
    "acc_test.append(metrics.accuracy_score(titanic_test_target,sv1.predict(titanic_test_inputs)))\n",
    "acc_test.append(metrics.accuracy_score(titanic_test_target,sv2.predict(titanic_test_inputs)))\n",
    "acc_test.append(metrics.accuracy_score(titanic_test_target,sv3.predict(titanic_test_inputs)))\n",
    "acc_test.append(metrics.accuracy_score(titanic_test_target,kn1.predict(titanic_test_inputs)))\n",
    "acc_test.append(metrics.accuracy_score(titanic_test_target,kn2.predict(titanic_test_inputs)))\n",
    "acc_test.append(metrics.accuracy_score(titanic_test_target,ad1.predict(titanic_test_inputs)))\n",
    "acc_test.append(metrics.accuracy_score(titanic_test_target,nb1.predict(titanic_test_inputs)))\n",
    "\n",
    "auc_test_df = pd.DataFrame(auc_test, index=auc_test_names, columns=['AUC'])\n",
    "acc_test_df = pd.DataFrame(acc_test, index=auc_test_names, columns=['Accuracy'])\n",
    "auc_test_df.join(acc_test_df).sort_values(by='AUC', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpretable Modeling Approach ---------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run DQR again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dqr(titanic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dqr(titanic_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temp2 = pd.DataFrame(titanic_raw.groupby(['Pclass'])['Survived'].mean())\n",
    "temp3 = pd.DataFrame(titanic_raw.groupby(['Pclass'])['Survived'].count())\n",
    "pd.DataFrame(temp2['Survived'].values, index=temp2.index, columns=['mean']) \\\n",
    "    .join(pd.DataFrame(temp3['Survived'].values, index=temp3.index, columns=['count']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temp2 = pd.DataFrame(titanic_raw.groupby(['Sex'])['Survived'].mean())\n",
    "temp3 = pd.DataFrame(titanic_raw.groupby(['Sex'])['Survived'].count())\n",
    "pd.DataFrame(temp2['Survived'].values, index=temp2.index, columns=['mean']) \\\n",
    "    .join(pd.DataFrame(temp3['Survived'].values, index=temp3.index, columns=['count']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temp = pd.DataFrame(titanic_raw.Cabin.str.get(0)).join(titanic_raw['Survived'])\n",
    "temp.loc[ (temp.Cabin.isnull()) , 'Cabin'] = \"NULL\"\n",
    "\n",
    "\n",
    "temp2 = pd.DataFrame(temp.groupby(['Cabin'])['Survived'].mean())\n",
    "temp3 = pd.DataFrame(temp.groupby(['Cabin'])['Survived'].count())\n",
    "pd.DataFrame(temp2['Survived'].values, index=temp2.index, columns=['mean']) \\\n",
    "    .join(pd.DataFrame(temp3['Survived'].values, index=temp3.index, columns=['count']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temp = pd.DataFrame(titanic_raw['Embarked']).join(titanic_raw['Survived'])\n",
    "temp.loc[ (temp.Embarked.isnull()), 'Embarked'] = \"NULL\"\n",
    "\n",
    "temp2 = pd.DataFrame(temp.groupby(['Embarked'])['Survived'].mean())\n",
    "temp3 = pd.DataFrame(temp.groupby(['Embarked'])['Survived'].count())\n",
    "pd.DataFrame(temp2['Survived'].values, index=temp2.index, columns=['mean']) \\\n",
    "    .join(pd.DataFrame(temp3['Survived'].values, index=temp3.index, columns=['count']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temp = pd.DataFrame(titanic_raw['Name']).join(titanic_raw['Survived']).join(titanic_raw['Age'])\n",
    "temp['Title']=\"NONE\"\n",
    "temp.loc[ (temp.Name.str.contains(\"Mr. \")), 'Title'] = \"Mr\"\n",
    "temp.loc[ (temp.Name.str.contains(\"Mrs. \")), 'Title'] = \"Mrs\"\n",
    "temp.loc[ (temp.Name.str.contains(\"Miss. \")), 'Title'] = \"Miss\"\n",
    "temp.loc[ (temp.Name.str.contains(\"Master. \")), 'Title'] = \"Master\"\n",
    "temp.loc[ (temp.Name.str.contains(\"Rev. \")), 'Title'] = \"Rev\"\n",
    "temp.loc[ (temp.Name.str.contains(\"Mme. \")), 'Title'] = \"Mme\"\n",
    "\n",
    "temp2 = pd.DataFrame(temp.groupby(['Title'])['Survived'].mean())\n",
    "temp3 = pd.DataFrame(temp.groupby(['Title'])['Survived'].count())\n",
    "temp4 = pd.DataFrame(temp.groupby(['Title'])['Age'].min())\n",
    "temp5 = pd.DataFrame(temp.groupby(['Title'])['Age'].max())\n",
    "pd.DataFrame(temp2['Survived'].values, index=temp2.index, columns=['mean']) \\\n",
    "    .join(pd.DataFrame(temp3['Survived'].values, index=temp3.index, columns=['count'])) \\\n",
    "    .join(pd.DataFrame(temp4['Age'].values, index=temp3.index, columns=['min_age'])) \\\n",
    "    .join(pd.DataFrame(temp5['Age'].values, index=temp4.index, columns=['max_age']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temp = pd.DataFrame(titanic_raw['Name']).join(titanic_raw['Survived']).join(titanic_raw['Age'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for multicolinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pandas.tools.plotting import scatter_matrix\n",
    "\n",
    "scatter_matrix(titanic[['Age', 'Fare', 'FamilySize']], alpha=0.2, figsize=(6, 6), diagonal='kde')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!pip install lime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import lime\n",
    "from lime import lime_tabular\n",
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv_train, cv_test, cv_labels_train, cv_labels_test = sk.cross_validation.train_test_split(titanic_inputs, titanic_target, train_size=0.80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "explainer = lime.lime_tabular.LimeTabularExplainer(cv_train, mode=\"classification\", class_names=['Died', 'Survived'],\n",
    "                                                   feature_names=columns, verbose=True,   \n",
    "                                                   discretize_continuous=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "i = np.random.randint(0, cv_test.shape[0])\n",
    "i = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv_test[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('True Survived Label', cv_labels_test[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Predicted Probability of Surviving', rf2.predict_proba(cv_test[i].reshape(1,-1))[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "exp = explainer.explain_instance(cv_test[i], rf2.predict_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "exp.show_in_notebook(show_table=True, show_all=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "exp = explainer.explain_instance(cv_test[i], rf2.predict_proba, num_features=5)\n",
    "print('Couples probability of staying together:', exp.predict_proba[1])\n",
    "exp.as_pyplot_figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
